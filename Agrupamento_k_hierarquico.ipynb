{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8bc938",
   "metadata": {},
   "source": [
    "# Agrupamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4ee00",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc1609",
   "metadata": {},
   "source": [
    "O K-means é um algoritmo do tipo **não supervisionado**, com objetivo de encontrar similaridades entre os dados e agrupá-los conforme o número de cluster passado pelo argumento k, agrupando os elementos próximos ao mesmo cluster.\n",
    "\n",
    "Pode-se inicializar os centroides de forma aleatória, passando um posição inicial ou levando em conta a posição de n elementos pré estabelecidos\n",
    "\n",
    "- 'k-means ++': seleciona centros de cluster iniciais para o cluster de k-mean de uma forma inteligente para acelerar a convergência. Veja a seção Notas em k_init para mais detalhes.\n",
    "\n",
    "- 'random': escolha n_clustersobservações (linhas) aleatoriamente de dados para os centróides iniciais.\n",
    "\n",
    "- Se um array for passado, ele deve ter o formato (n_clusters, n_features) e fornecer os centros iniciais.\n",
    "\n",
    "Por levar em conta a distância e agrupar os elementos mais próximos, recalculando a posição dos centroides a cada interação, é sensitivel à outliers, uma vez que cada centroide representa a media das distâncias entre todos os elementos, sendo o centro do cluster.\n",
    "\n",
    "\n",
    "<img src='imagens_agrupamento/centroids.png'>\n",
    "\n",
    "\n",
    "<img src='imagens_agrupamento/kmeans.gif'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95511f69",
   "metadata": {},
   "source": [
    "## K-medoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78ac56",
   "metadata": {},
   "source": [
    "O k-medoid é baseado no medoids, que é **um ponto que pertence ao conjunto de dados**, calculado minimizando a distância absoluta entre os pontos e o centróide selecionado, em vez de minimizar a distância quadrada. Como resultado, **é mais robusto a ruídos e outliers do que o k-means**.\n",
    "\n",
    "Pelo fato do K-medoid utilizar um ponto ao em vez da distância média, é melhor que o K-means, porém tem uma função de custo muito maior, uma vez que em cada interação ele calcula o novo medoide olhando todos para todos, pegando o medoide que possui menor distância em relação aos demais\n",
    "\n",
    "<img src='imagens_agrupamento/kmedoid.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827d3b9",
   "metadata": {},
   "source": [
    "## K-medians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dfac64",
   "metadata": {},
   "source": [
    "É uma variação do agrupamento de k-means em que, em vez de calcular a média de cada agrupamento para determinar seu **centróide**, calcula-se a **mediana**.\n",
    "\n",
    "**A mediana é calculada em cada dimensão na formulação da distância de Manhattan** do problema de k-medianas, de modo que os atributos individuais virão do conjunto de dados. Isso torna o algoritmo **mais confiável** para conjuntos de dados discretos ou até binários. Em contraste, o uso de meios ou medianas de distância euclidiana não produzirá necessariamente atributos individuais do conjunto de dados.\n",
    "\n",
    "Um medóide tem que ser uma instância real do conjunto de dados, enquanto para a mediana da distância de Manhattan, isso não ocorre necessáriamente, como mostra o exemplo abaixo\n",
    "\n",
    "Sendo $a=(0,1)$, $b=(1,0)$ e $c=(2,2)$, a mediana da distância de Manhattan é $(1,1)$\n",
    "\n",
    "Esse valor não existe nos dados originais e, portanto, não pode ser um medóide.\n",
    "\n",
    "\n",
    "**Obs**: A distância de Manhattan ou a distância taxi é dada pelo exemplo abaixo:\n",
    "\n",
    "<img src='imagens_agrupamento/manhattan.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366294f",
   "metadata": {},
   "source": [
    "## Agrupamento Hierárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7fb2a",
   "metadata": {},
   "source": [
    "A ideia do agrupamento hierárquico é gerar uma hierarquia entre os dados baseados na distância entre os dados, gerando um dendograma.\n",
    "\n",
    "<img src='imagens_agrupamento/dendograma.png'>\n",
    "\n",
    "Além disso, pode nos mostrar a presença de outliers, uma vez que leva a distância euclidiana em consideração.\n",
    "\n",
    "<img src='imagens_agrupamento/hierarquico2.png'>\n",
    "\n",
    "<img src='imagens_agrupamento/resumo.jpg'>\n",
    "\n",
    "\n",
    "### Single Linkage\n",
    "\n",
    "A distância entre dois grupos é dada pela **menor distância** entre um objeto do grupo 1 e um outro objeto do grupo 2 e consegue clusterizar grupos não globulares \n",
    "\n",
    "<img src='imagens_agrupamento/single.png'>\n",
    "\n",
    "\n",
    "### Complete Linkage\n",
    "\n",
    "A distância entre dois grupos é dada pela **maior distância** entre um objeto do grupo 1 e um outro objeto do grupo 2\n",
    "\n",
    "<img src='imagens_agrupamento/complete.png'>\n",
    "\n",
    "\n",
    "### Average Linkage\n",
    "\n",
    "A distância entre dois grupos é dada pela **média distância** entre um objeto do grupo 1 e um outro objeto do grupo 2. Ou seja, **calcula-se a distância de cada objeto em um grupo para todos os objetos do outro grupo e divide pelo número de objetos de ambos os grupos**\n",
    "\n",
    "- Acaba se comportando de forma parecida com o Ward.\n",
    "\n",
    "<img src='imagens_agrupamento/average.png'>\n",
    "\n",
    "<img src='imagens_agrupamento/average2.png'>\n",
    "\n",
    "\n",
    "### Ward Linkage\n",
    "\n",
    "O método de variância mínima de **Ward** leva como critério para escolher o par de clusters a fundir em cada etapa o valor ótimo de uma função objetivo, que seria a **soma dos quadrados (ss) da distância entre o ponto e a média**, a fim de manter a mínima variância possível, no qual SS é dado por:\n",
    "\n",
    "Variância: é a distancia média entre cada ponto e a média da amostra\n",
    "\n",
    "$$\n",
    "Var = \\frac{(x1 - média)² + (x2 - média)² + (xn - média)²}{n}\n",
    "$$\n",
    "$$\n",
    "SS = (x - média)²\n",
    "$$\n",
    "$$\n",
    "SSrezultante = \\sum_{j=0}^{nClusters}(SS) \n",
    "$$\n",
    "\n",
    "Para agrupar grupos, a distância entre esses grupos é dada pela média ponderada entre os dois grupos, ou seja:\n",
    "\n",
    "$$\n",
    "D = \\frac{nGrupo1*nGrupo2}{nGrupo1 + nGrupo2}*(Média\\:do\\:grupo1 - Média\\:do\\:grupo2)\n",
    "$$\n",
    "\n",
    "\n",
    "- Tende a criar clusters igualmente distribuídos, ou seja, muitas vezes com forma arredondada.\n",
    "\n",
    "\n",
    "### Centroid Linkage\n",
    "\n",
    "Usa as distâncias entre os centroides como métrica, no qual o centroide é o ponto médio de todos os pontos do grupo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
